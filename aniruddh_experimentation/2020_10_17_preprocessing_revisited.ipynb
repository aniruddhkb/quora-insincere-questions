{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit760f76561f6442a8b7e1ff17f4562bdb",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2020_10_17 Preprocessing Revisited\n",
    "\n",
    "The purpose of this notebook is to revisit the preprocessing steps from start to finish in order to make a unified pipeline. The aim is to reduce the dimensionality of the TDFIDF/CountVectorizer matrix to the greatest extent possible."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSET_FOLDER_PATH = './dataset/quora/'\n",
    "CORPUS_FOLDER_PATH = './corpi/'\n",
    "import nltk \n",
    "import re \n",
    "import contractions \n",
    "import jamspell\n",
    "import pandas as pd\n",
    "import wordninja\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df = pd.read_csv(DSET_FOLDER_PATH + \"train.csv\")\n",
    "test_dset_df = pd.read_csv(DSET_FOLDER_PATH + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor_2020_10_17:\n",
    "    def __init__(self, jamspell_corpus,word_term=0, freq_term=1, separator=\" \", stemmer=\"snowball\"):\n",
    "        '''\n",
    "        Parameters:\n",
    "            symspell_corpus: path to textfile of word-frequency pairs.\n",
    "        '''\n",
    "        self.tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "        self.spellChecker = jamspell.TSpellCorrector()\n",
    "        self.spellChecker.LoadLangModel(jamspell_corpus) \n",
    "        self.stopwordCorpus = set(nltk.corpus.stopwords.words())\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.nltk_tag_to_wordnet_tag = {'J':nltk.corpus.wordnet.ADJ, 'V':nltk.corpus.wordnet.VERB, 'N':nltk.corpus.wordnet.NOUN, 'R':nltk.corpus.wordnet.ADJ}\n",
    "        if(stemmer == \"porter\"):\n",
    "            self.stemmer = nltk.stem.PorterStemmer()\n",
    "        elif(stemmer == \"snowball\"):\n",
    "            self.stemmer = nltk.SnowballStemmer(\"english\")\n",
    "        elif(stemmer == \"lancaster\"):\n",
    "            self.stemmer = nltk.LancasterStemmer()\n",
    "        else:\n",
    "            print(\"Error. Incorrect keyword passed for stemmer.\")\n",
    "            raise Exception\n",
    "    def preprocess(self, sentence, spellcheck= True, stopword_removal = True, lemmatization=True, stemming=True):\n",
    "        '''\n",
    "        A string\n",
    "        '''\n",
    "        sentence= sentence.lower() #1\n",
    "        if(spellcheck):\n",
    "            sentence = self.spellChecker.FixFragment(sentence)\n",
    "        sentence= contractions.fix(sentence) #2 \n",
    "        tokenized_sentence= self.tokenizer.tokenize(sentence) #3\n",
    "        tokenized_sentence= [''.join([i for i in s if i.isalpha()])for s in tokenized_sentence] #4\n",
    "        if(spellcheck):\n",
    "            new_sentence = []\n",
    "            for word in tokenized_sentence:\n",
    "                new_sentence += wordninja.split(word)\n",
    "            tokenized_sentence = new_sentence\n",
    "        tokenized_sentence= [i for i in tokenized_sentence if len(i) > 0] #4\n",
    "        if(stopword_removal):\n",
    "            tokenized_sentence= [word for word in tokenized_sentence if not word in self.stopwordCorpus]\n",
    "        if(lemmatization):\n",
    "            tokenized_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "            tokenized_sentence = [(word[0], self.nltk_tag_to_wordnet_tag.get(word[1][0] if len(word[1]) > 0 else None, nltk.corpus.wordnet.NOUN)) for word in tokenized_sentence]\n",
    "            tokenized_sentence = [self.lemmatizer.lemmatize(word[0], pos=word[1]) for word in tokenized_sentence]\n",
    "        if(stemming):\n",
    "            tokenized_sentence = [self.stemmer.stem(word) for word in tokenized_sentence]\n",
    "        return tokenized_sentence\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor_2020_10_17(CORPUS_FOLDER_PATH + \"en.bin\", stemmer=\"snowball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 783673/783673 [27:40<00:00, 472.06it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dset_df[\"preprocessed\"] = train_dset_df[\"question_text\"].progress_apply(lambda x: pp.preprocess(x, lemmatization=True, stemming=True, stopword_removal=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 522449/522449 [18:22<00:00, 473.74it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dset_df[\"preprocessed\"] = test_dset_df[\"question_text\"].progress_apply(lambda x: pp.preprocess(x, lemmatization=True, stemming=True, stopword_removal=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 783673 entries, 0 to 783672\nData columns (total 4 columns):\n #   Column         Non-Null Count   Dtype \n---  ------         --------------   ----- \n 0   qid            783673 non-null  object\n 1   question_text  783673 non-null  object\n 2   target         783673 non-null  int64 \n 3   preprocessed   783673 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 23.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_dset_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 783673/783673 [00:00<00:00, 1312118.37it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dset_df[\"preprocessed_joined\"] =  train_dset_df.preprocessed.progress_apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 522449/522449 [00:00<00:00, 1284054.68it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dset_df[\"preprocessed_joined\"] =  test_dset_df.preprocessed.progress_apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 783673 entries, 0 to 783672\nData columns (total 5 columns):\n #   Column               Non-Null Count   Dtype \n---  ------               --------------   ----- \n 0   qid                  783673 non-null  object\n 1   question_text        783673 non-null  object\n 2   target               783673 non-null  int64 \n 3   preprocessed         783673 non-null  object\n 4   preprocessed_joined  783673 non-null  object\ndtypes: int64(1), object(4)\nmemory usage: 29.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_dset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 522449 entries, 0 to 522448\nData columns (total 4 columns):\n #   Column               Non-Null Count   Dtype \n---  ------               --------------   ----- \n 0   qid                  522449 non-null  object\n 1   question_text        522449 non-null  object\n 2   preprocessed         522449 non-null  object\n 3   preprocessed_joined  522449 non-null  object\ndtypes: object(4)\nmemory usage: 15.9+ MB\n"
     ]
    }
   ],
   "source": [
    "test_dset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df.drop(inplace=True, axis=\"columns\", labels =[\"question_text\", \"preprocessed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df.drop(inplace=True, axis=\"columns\", labels =[\"question_text\", \"preprocessed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 783673 entries, 0 to 783672\nData columns (total 3 columns):\n #   Column               Non-Null Count   Dtype \n---  ------               --------------   ----- \n 0   qid                  783673 non-null  object\n 1   target               783673 non-null  int64 \n 2   preprocessed_joined  783673 non-null  object\ndtypes: int64(1), object(2)\nmemory usage: 17.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_dset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 522449 entries, 0 to 522448\nData columns (total 2 columns):\n #   Column               Non-Null Count   Dtype \n---  ------               --------------   ----- \n 0   qid                  522449 non-null  object\n 1   preprocessed_joined  522449 non-null  object\ndtypes: object(2)\nmemory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "test_dset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df.to_csv(\"2020_10_18_train_dset_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df.to_csv(\"2020_10_18_test_dset_df.csv\", index=False)"
   ]
  },
  {
   "source": [
    "# Building a dummy model for comparison"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "vectorizer.fit(train_dset_df[\"preprocessed_joined\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_x_train = vectorizer.transform(train_dset_df[\"preprocessed_joined\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sparse_x_train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5bbcfdfdf7c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparse_x_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sparse_x_train' is not defined"
     ]
    }
   ],
   "source": [
    "sparse_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = BernoulliNB(class_prior=(0.95, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BernoulliNB(class_prior=(0.95, 0.05))"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "mnb.fit(sparse_x_train, train_dset_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = mnb.predict(sparse_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_dset_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5378409739745454"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "f1_score(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df[\"yhat\"] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongs = train_dset_df[train_dset_df[\"yhat\"] != train_dset_df[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, 30934), (1, 19250)]"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "[(name, len(x)) for name, x in wrongs.groupby(by=\"target\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, 735222), (1, 48451)]"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "[(name, len(x)) for name, x in train_dset_df.groupby(by=\"target\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df.to_csv(\"2020_10_18_train_dset_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df.to_csv(\"2020_10_18_test_dset_df.csv\")"
   ]
  },
  {
   "source": [
    "# Building an SVM Model for further testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(verbose=2, max_iter = 4000, class_weight= {0:1, 1:8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVC(class_weight={0: 1, 1: 4}, max_iter=4000, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "svm.fit(sparse_x_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = svm.predict(sparse_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_dset_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df[\"yhat\"] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongs = train_dset_df[train_dset_df[\"yhat\"] != train_dset_df[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6448837041837691"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "f1_score(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, 26823), (1, 12629)]"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "[(name, len(x)) for name, x in wrongs.groupby(by=\"target\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, 735222), (1, 48451)]"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "[(name, len(x)) for name, x in train_dset_df.groupby(by=\"target\")]"
   ]
  },
  {
   "source": [
    "# Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_x = vectorizer.transform(test_dset_df[\"preprocessed_joined\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = svm.predict(sparse_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df.question_text = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    qid  target\n",
       "0  f56a9a31974dc66186e8       0\n",
       "1  d957c3758060f45da303       0\n",
       "2  ad822d5abaedb9e247b9       0\n",
       "3  4e979c23eeb6a4bd1f2e       0\n",
       "4  333cc031262566b8da49       0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f56a9a31974dc66186e8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d957c3758060f45da303</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ad822d5abaedb9e247b9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4e979c23eeb6a4bd1f2e</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>333cc031262566b8da49</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "test_dset_df = test_dset_df.drop(axis=\"columns\", labels=\"preprocessed\")\n",
    "test_dset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df = test_dset_df.rename(columns={\"qid\":\"qid\", \"question_text\":\"target\"})\n",
    "test_dset_df.target = test_dset_df.target.apply(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df.to_csv(\"./outputs/2020_10_18_a_testset_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}