{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit760f76561f6442a8b7e1ff17f4562bdb",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimizing preprocessing quality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSET_FOLDER_PATH = './dataset/quora/'\n",
    "GLOVE_FOLDER_PATH = './embeddings/glove/'\n",
    "CORPUS_FOLDER_PATH = './corpi/'\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt  \n",
    "import wordcloud as wc \n",
    "import seaborn as sns \n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import symspellpy\n",
    "train_dset_df = pd.read_csv(DSET_FOLDER_PATH + \"train.csv\")\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Preprocessor:\n",
    "#     def __init__(self, path_to_words_corpus):\n",
    "#         self.sym_spell = symspellpy.SymSpell()\n",
    "#         self.sym_spell.create_dictionary(path_to_words_corpus)\n",
    "#         self.tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "#         self.stopwords_corpus = set(nltk.corpus.stopwords.words())\n",
    "#         self.stemmer = nltk.stem.PorterStemmer()\n",
    "#     def preprocess(self,sentence, remove_stopwords=True, stem_reduce=True):\n",
    "#         sentence = sentence.lower()\n",
    "#         sentence = re.sub(r\"\\d+\", \"\", sentence)\n",
    "#         sentence = contractions.fix(sentence)\n",
    "#         sentence = self.tokenizer.tokenize(sentence)\n",
    "#         if(remove_stopwords):\n",
    "#             sentence = [word for word in sentence if not word in self.stopwords_corpus]\n",
    "#         if(stem_reduce):\n",
    "#             sentence = [self.stemmer.stem(word) for word in sentence]\n",
    "#         sentence = [self.sym_spell.lookup(word, 0, include_unknown=True)[0].term for word in sentence]\n",
    "#         return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessor = Preprocessor(CORPUS_FOLDER_PATH + \"words_alpha.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dset_df[\"preprocessed\"] = train_dset_df.question_text.progress_apply(lambda x: preprocessor.preprocess(x, stem_reduce=False))\n",
    "# train_dset_df[\"preprocessed_joined\"] = train_dset_df.preprocessed.progress_apply(lambda x: \" \".join(x))\n",
    "# string_of_all_words = \" \".join(train_dset_df.preprocessed_joined.to_list())\n",
    "# list_of_all_words = string_of_all_words.split()\n",
    "# set_of_all_words = set(list_of_all_words)\n",
    "# len(set_of_all_words)\n",
    "# string_of_all_words = \" \".join(sorted(list(set_of_all_words)))\n",
    "# file = open(\"2020_10_04_unique_words_0.txt\", \"w\")\n",
    "# file.write(string_of_all_words)\n",
    "# file.close()"
   ]
  },
  {
   "source": [
    "Result: The preprocessing is generally good, but lemmatization is needed. In order to do this lemmatization, let us do the tagging."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, path_to_words_corpus):\n",
    "        self.sym_spell = symspellpy.SymSpell()\n",
    "        self.sym_spell.create_dictionary(path_to_words_corpus)\n",
    "        self.tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "        self.stopwords_corpus = set(nltk.corpus.stopwords.words())\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.nltk_tag_to_wordnet_tag = {'J':nltk.corpus.wordnet.ADJ, 'V':nltk.corpus.wordnet.VERB, 'N':nltk.corpus.wordnet.NOUN, 'R':nltk.corpus.wordnet.ADJ}\n",
    "    def preprocess(self,sentence, remove_stopwords=True, lemmatize=True):\n",
    "        sentence = re.sub(r\"\\d+\", \"\", sentence)\n",
    "        sentence = contractions.fix(sentence)\n",
    "        sentence = self.tokenizer.tokenize(sentence)\n",
    "        if(remove_stopwords):\n",
    "            sentence = [word for word in sentence if not word in self.stopwords_corpus]\n",
    "        \n",
    "        if(lemmatize):\n",
    "            sentence = nltk.pos_tag(sentence)\n",
    "            sentence = [(word[0], self.nltk_tag_to_wordnet_tag.get(word[1][0] if len(word[1]) > 0 else None, nltk.corpus.wordnet.NOUN)) for word in sentence]\n",
    "            sentence = [self.lemmatizer.lemmatize(word[0], pos=word[1]) for word in sentence]\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [self.sym_spell.lookup(word, 0, include_unknown=True)[0].term for word in sentence]\n",
    "        return sentence\n",
    "preprocessor = Preprocessor(CORPUS_FOLDER_PATH + \"words_alpha.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = train_dset_df[\"question_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;how&#39;,\n &#39;i&#39;,\n &#39;reply&#39;,\n &#39;comment&#39;,\n &#39;india&#39;,\n &#39;poor&#39;,\n &#39;it&#39;,\n &#39;fact&#39;,\n &#39;i&#39;,\n &#39;understand&#39;,\n &#39;unnecessary&#39;,\n &#39;criticism&#39;,\n &#39;snapshot&#39;,\n &#39;cero&#39;,\n &#39;statement&#39;]"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "preprocessor.preprocess(sample_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 783673/783673 [09:03&lt;00:00, 1442.64it/s]\n"
    }
   ],
   "source": [
    "train_dset_df[\"preprocessed\"] = train_dset_df.question_text.progress_apply(lambda x: preprocessor.preprocess(x))\n",
    "train_dset_df.to_csv(\"2020_10_04_preprocessed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 522449/522449 [05:47&lt;00:00, 1501.79it/s]\n"
    }
   ],
   "source": [
    "test_dset_df = pd.read_csv(DSET_FOLDER_PATH + \"test.csv\")\n",
    "test_dset_df[\"preprocessed\"] = test_dset_df.question_text.progress_apply(lambda x: preprocessor.preprocess(x))\n",
    "test_dset_df.to_csv(\"2020_10_04_preprocessed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}