{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit760f76561f6442a8b7e1ff17f4562bdb",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSET_FOLDER_PATH = './dataset/quora/'\n",
    "CORPUS_FOLDER_PATH = './corpi/'\n",
    "import nltk \n",
    "import re \n",
    "import contractions \n",
    "import jamspell\n",
    "import pandas as pd\n",
    "import wordninja\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df = pd.read_csv(DSET_FOLDER_PATH + \"train.csv\")\n",
    "test_dset_df = pd.read_csv(DSET_FOLDER_PATH + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor_2020_10_17:\n",
    "    def __init__(self, jamspell_corpus,word_term=0, freq_term=1, separator=\" \", stemmer=\"snowball\"):\n",
    "        '''\n",
    "        Parameters:\n",
    "            symspell_corpus: path to textfile of word-frequency pairs.\n",
    "        '''\n",
    "        self.tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "        self.spellChecker = jamspell.TSpellCorrector()\n",
    "        self.spellChecker.LoadLangModel(jamspell_corpus) \n",
    "        self.stopwordCorpus = set(nltk.corpus.stopwords.words())\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.nltk_tag_to_wordnet_tag = {'J':nltk.corpus.wordnet.ADJ, 'V':nltk.corpus.wordnet.VERB, 'N':nltk.corpus.wordnet.NOUN, 'R':nltk.corpus.wordnet.ADJ}\n",
    "        if(stemmer == \"porter\"):\n",
    "            self.stemmer = nltk.stem.PorterStemmer()\n",
    "        elif(stemmer == \"snowball\"):\n",
    "            self.stemmer = nltk.SnowballStemmer(\"english\")\n",
    "        elif(stemmer == \"lancaster\"):\n",
    "            self.stemmer = nltk.LancasterStemmer()\n",
    "        else:\n",
    "            print(\"Error. Incorrect keyword passed for stemmer.\")\n",
    "            raise Exception\n",
    "    def preprocess(self, sentence, spellcheck= True, stopword_removal = True, lemmatization=True, stemming=True):\n",
    "        '''\n",
    "        A string\n",
    "        '''\n",
    "        sentence= sentence.lower() #1\n",
    "        if(spellcheck):\n",
    "            sentence = self.spellChecker.FixFragment(sentence)\n",
    "        sentence= contractions.fix(sentence) #2 \n",
    "        tokenized_sentence= self.tokenizer.tokenize(sentence) #3\n",
    "        tokenized_sentence= [''.join([i for i in s if i.isalpha()])for s in tokenized_sentence] #4\n",
    "        if(spellcheck):\n",
    "            new_sentence = []\n",
    "            for word in tokenized_sentence:\n",
    "                new_sentence += wordninja.split(word)\n",
    "            tokenized_sentence = new_sentence\n",
    "        tokenized_sentence= [i for i in tokenized_sentence if len(i) > 0] #4\n",
    "        if(stopword_removal):\n",
    "            tokenized_sentence= [word for word in tokenized_sentence if not word in self.stopwordCorpus]\n",
    "        if(lemmatization):\n",
    "            tokenized_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "            tokenized_sentence = [(word[0], self.nltk_tag_to_wordnet_tag.get(word[1][0] if len(word[1]) > 0 else None, nltk.corpus.wordnet.NOUN)) for word in tokenized_sentence]\n",
    "            tokenized_sentence = [self.lemmatizer.lemmatize(word[0], pos=word[1]) for word in tokenized_sentence]\n",
    "        if(stemming):\n",
    "            tokenized_sentence = [self.stemmer.stem(word) for word in tokenized_sentence]\n",
    "        return tokenized_sentence\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocessor_2020_10_17(CORPUS_FOLDER_PATH + \"en.bin\", stemmer=\"snowball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 783673/783673 [27:47<00:00, 469.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dset_df[\"preprocessed\"] = train_dset_df[\"question_text\"].progress_apply(lambda x: pp.preprocess(x, lemmatization=True, stemming=False, stopword_removal=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 522449/522449 [18:30<00:00, 470.55it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dset_df[\"preprocessed\"] = test_dset_df[\"question_text\"].progress_apply(lambda x: pp.preprocess(x, lemmatization=True, stemming=False, stopword_removal=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 783673/783673 [00:00<00:00, 1121098.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dset_df[\"preprocessed_joined\"] =  train_dset_df.preprocessed.progress_apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 522449/522449 [00:00<00:00, 1140127.94it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dset_df[\"preprocessed_joined\"] =  test_dset_df.preprocessed.progress_apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df.drop(inplace=True, axis=\"columns\", labels =[\"question_text\", \"preprocessed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df.drop(inplace=True, axis=\"columns\", labels =[\"question_text\", \"preprocessed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df.to_csv(\"2020_10_19_train_dset_df_nostem_nostoprem.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df.to_csv(\"2020_10_19_test_dset_df_nostem_nostoprem.csv\", index=False)"
   ]
  }
 ]
}