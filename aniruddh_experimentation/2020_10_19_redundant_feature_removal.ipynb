{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit760f76561f6442a8b7e1ff17f4562bdb",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Attempt at removing redundant columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Conclusion - every feature is (somehow) useful -- none can be removed at this stage."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Imports and vectorization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "train_dset_df = pd.read_csv(\"2020_10_19_train_dset_df_nostem_nostoprem.csv\")\n",
    "test_dset_df = pd.read_csv(\"2020_10_19_test_dset_df_nostem_nostoprem.csv\")\n",
    "train_dset_df[\"preprocessed_joined\"].fillna(\"\", inplace=True)\n",
    "test_dset_df[\"preprocessed_joined\"].fillna(\"\", inplace=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_dset_df[\"preprocessed_joined\"])\n",
    "sparse_train_x = vectorizer.transform(train_dset_df[\"preprocessed_joined\"])\n",
    "sparse_test_x  = vectorizer.transform(test_dset_df[\"preprocessed_joined\"])\n",
    "train_dset_y = train_dset_df[\"target\"].to_numpy()\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Removing redundant columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, let us remove those columns which have virtually no predictive power."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_x_csc = sparse_train_x.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 54972/54972 [05:16<00:00, 173.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dset_y = train_dset_y.reshape(-1, 1)\n",
    "n_y = np.sum(train_dset_y)\n",
    "n_all = sparse_train_x_csc.shape[0]\n",
    "p_y = n_y/n_all\n",
    "probability_epsilon = 0.05\n",
    "to_keep = []\n",
    "for column_id in tqdm(range(sparse_train_x_csc.shape[1])):\n",
    "    curr_col = sparse_train_x_csc[:,column_id]\n",
    "    curr_col = ((curr_col > 0).astype(int)).toarray().reshape(-1, 1)\n",
    "    n_curr_col = curr_col.sum()\n",
    "    # print(\"n_curr_col:\",n_curr_col)\n",
    "    n_not_curr_col = n_all - n_curr_col\n",
    "    # print(\"n_not_curr_col:\",n_not_curr_col)\n",
    "    n_y_and_curr_col = np.sum((np.logical_and(train_dset_y, curr_col)).astype(int))\n",
    "    # print(\"n_y_andcurr_col:\",n_y_and_curr_col)\n",
    "    n_y_and_not_curr_col = np.sum(np.sum(np.logical_and(train_dset_y, np.logical_not(curr_col))).astype(int))\n",
    "    # print(\"n_y_and_not_curr_col:\",n_y_and_not_curr_col)\n",
    "    p_y_and_curr_col = n_y_and_curr_col/n_curr_col \n",
    "    p_y_and_not_curr_col = n_y_and_not_curr_col/n_not_curr_col\n",
    "    if(abs(p_y_and_curr_col - p_y) > probability_epsilon or abs(p_y_and_not_curr_col - p_y) > probability_epsilon):\n",
    "        to_keep.append(column_id)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(45973, 54972)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "len(to_keep), sparse_train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_x_reduced = sparse_train_x[:, to_keep]"
   ]
  },
  {
   "source": [
    "## LinearSVC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVC(C=0.01, class_weight={0: 1, 1: 4})"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "svm = LinearSVC(penalty=\"l2\",dual=True,class_weight={0:1,1:4}, C=0.01)\n",
    "svm.fit(sparse_train_x_reduced, train_dset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6223723232077537"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "train_dset_yhat = svm.predict(sparse_train_x_reduced)\n",
    "f1_score(train_dset_y, train_dset_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25177 15188\n735222 48451\n"
     ]
    }
   ],
   "source": [
    "train_dset_df[\"yhat\"] = train_dset_yhat\n",
    "wrongs = train_dset_df[train_dset_df[\"target\"] != train_dset_df[\"yhat\"]]\n",
    "print(len(wrongs.groupby(by=\"target\").get_group(0)),len(wrongs.groupby(by=\"target\").get_group(1)))\n",
    "print(len(train_dset_df.groupby(by=\"target\").get_group(0)), len(train_dset_df.groupby(by=\"target\").get_group(1)))"
   ]
  },
  {
   "source": [
    "## Cross-validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAINING AGAIN. 0\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 1\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 2\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 3\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 4\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 5\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 6\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 7\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 8\n",
      "0\n",
      "0\n",
      "0\n",
      "TRAINING AGAIN. 9\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "kfcv = KFold(n_splits=10, shuffle=True)\n",
    "train_f1_scores = []\n",
    "test_f1_scores = []\n",
    "i=0\n",
    "for train_index, test_index in kfcv.split(sparse_train_x_reduced):\n",
    "    print(\"TRAINING AGAIN.\", i)\n",
    "    i+=1\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = sparse_train_x_reduced[train_index], sparse_train_x_reduced[test_index]\n",
    "    y_train, y_test = train_dset_y[train_index], train_dset_y[test_index]\n",
    "    svm.fit(x_train, y_train)\n",
    "    train_yhat = svm.predict(x_train)\n",
    "    train_f1_score = f1_score(y_train, train_yhat)\n",
    "    test_yhat = svm.predict(x_test)\n",
    "    test_f1_score = f1_score(y_test, test_yhat)\n",
    "    train_f1_scores.append(train_f1_score)\n",
    "    test_f1_scores.append(test_f1_score)\n",
    "    train_yhat = None \n",
    "    test_yhat = None \n",
    "    x_train = None \n",
    "    x_test = None \n",
    "    y_train = None \n",
    "    y_test = None \n",
    "    print(gc.collect())\n",
    "    print(gc.collect())\n",
    "    print(gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.6211308037943086,\n",
       " 0.6228194346142637,\n",
       " 0.622641901437713,\n",
       " 0.6213503269607231,\n",
       " 0.6218340247232704,\n",
       " 0.6229157804085953,\n",
       " 0.6229228092474709,\n",
       " 0.6235417991514027,\n",
       " 0.6211023425038833,\n",
       " 0.623167475349881]"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "train_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.6117735423903858,\n",
       " 0.5933213508655757,\n",
       " 0.5969441017367372,\n",
       " 0.6084507042253521,\n",
       " 0.6061682242990654,\n",
       " 0.5965952563121654,\n",
       " 0.5987355894384531,\n",
       " 0.5949024367472692,\n",
       " 0.6070898292501856,\n",
       " 0.5946153109130977]"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "test_f1_scores"
   ]
  },
  {
   "source": [
    "## Testset write"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}