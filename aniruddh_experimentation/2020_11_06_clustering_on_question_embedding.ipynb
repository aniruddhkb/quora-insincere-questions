{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit760f76561f6442a8b7e1ff17f4562bdb",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Imports, classdefs, stopword removal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 783673/783673 [00:03<00:00, 248351.92it/s]\n"
     ]
    }
   ],
   "source": [
    "class Glove_Embedder:\n",
    "    def __init__(self, PATH_TO_TEXTFILE):\n",
    "        self.glove_embeddings_dict = {}\n",
    "        glove_embeddings_file = open(PATH_TO_TEXTFILE, 'r')\n",
    "        firstTime = True\n",
    "        while True:\n",
    "            line = glove_embeddings_file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            splitted = line.split()\n",
    "            key = splitted[0]\n",
    "            value = np.array([float(i) for i in splitted[1:]])\n",
    "            if(firstTime):\n",
    "                firstTime = False \n",
    "                self.embedding_vector_size = value.size\n",
    "            self.glove_embeddings_dict[key] = value\n",
    "        glove_embeddings_file.close()\n",
    "    def get_embedding_for_sentence(self, sentence_list):\n",
    "        # print(sentence_list)\n",
    "        '''\n",
    "        The sentence should be lowercased and free of special characters and numbers. Ideally, it should be lemmatized, too. The sentence should be a list of words.\n",
    "        '''\n",
    "        number_of_words = len(sentence_list)\n",
    "        embedding = np.zeros((self.embedding_vector_size, ))\n",
    "        if(number_of_words == 0):\n",
    "            return embedding \n",
    "        for word in sentence_list:\n",
    "            # print(word)\n",
    "            if word in self.glove_embeddings_dict:\n",
    "                embedding += self.glove_embeddings_dict[word]\n",
    "        embedding /= number_of_words\n",
    "        return embedding.tolist()\n",
    "    def get_embedding_for_word(self, word):\n",
    "        if word in self.glove_embeddings_dict:\n",
    "            embedding = self.glove_embeddings_dict[word]\n",
    "        else:\n",
    "            embedding = np.zeros((self.embedding_vector_size, ))\n",
    "        return embedding.tolist()\n",
    "\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gc\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "train_dset_df = pd.read_csv(\"2020_10_19_train_dset_df_nostem_nostoprem.csv\")\n",
    "test_dset_df = pd.read_csv(\"2020_10_19_test_dset_df_nostem_nostoprem.csv\")\n",
    "train_dset_df[\"preprocessed_joined\"].fillna(\"\", inplace=True)\n",
    "test_dset_df[\"preprocessed_joined\"].fillna(\"\", inplace=True)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_dset_df[\"preprocessed_joined\"])\n",
    "original_sparse_train_x = vectorizer.transform(train_dset_df[\"preprocessed_joined\"])\n",
    "original_sparse_test_x  = vectorizer.transform(test_dset_df[\"preprocessed_joined\"])\n",
    "train_dset_y = train_dset_df[\"target\"].to_numpy()\n",
    "\n",
    "def summarize(y, yhat):\n",
    "    '''\n",
    "    y and yhat are both 1-dimensional ndarrays where every entry is either 0 or 1. \n",
    "    y and yhat must have the same size \n",
    "    '''\n",
    "    print(\"Number of zeros in y:\", np.sum( (y == 0).astype(int) ))\n",
    "    print(\" Number of ones in y:\", np.sum((y == 1).astype(int)))\n",
    "    print(\"            F1 score:\", f1_score(y, yhat))\n",
    "    print(\" # of zeros wrong yh:\", np.sum(np.logical_and(y == 0, yhat == 1).astype(int)))\n",
    "    print(\"  # of ones wrong yh:\", np.sum(np.logical_and(y == 1, yhat == 0).astype(int)))\n",
    "\n",
    "embedder = Glove_Embedder(\"./embeddings/glove/glove.6B.50d.txt\")\n",
    "import nltk\n",
    "class Stopword_Remover:\n",
    "    def __init__(self):\n",
    "        self.stopwordCorpus = set(nltk.corpus.stopwords.words())\n",
    "    def stopword_removed(self, sentence_str):\n",
    "        return \" \".join([word for word in sentence_str.split(\" \") if not word in self.stopwordCorpus])\n",
    "srem = Stopword_Remover()\n",
    "train_dset_df[\"preprocessed_stopword_removed\"] = train_dset_df[\"preprocessed_joined\"].progress_apply(lambda x: (srem.stopword_removed(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_stopword_removed = train_dset_df[\"preprocessed_stopword_removed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = preprocessed_stopword_removed.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENTENCES = len(list_of_sentences)\n",
    "EMBEDDING_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((N_SENTENCES, EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 783673/783673 [00:20<00:00, 38788.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(N_SENTENCES)):\n",
    "    sentence = list_of_sentences[i]\n",
    "    embeddings[i,:] = embedder.get_embedding_for_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}