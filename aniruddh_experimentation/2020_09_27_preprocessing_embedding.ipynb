{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit760f76561f6442a8b7e1ff17f4562bdb",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Initial data preprocessing steps, mark one"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSET_FOLDER_PATH = './dataset/quora/'\n",
    "GLOVE_FOLDER_PATH = './embeddings/glove/'"
   ]
  },
  {
   "source": [
    "## 1. Collecting NLTK and the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset_df = pd.read_csv(DSET_FOLDER_PATH + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                    qid                                      question_text  \\\n0  6f47b0f60633c2056455  How can I reply to this comment, \"India is poo...   \n1  d49b3966070b27bf07fc  What did they use for transportation in Ancien...   \n2  6d5faa49380557c8ca7b  What are the most important provisions of Obam...   \n3  cebea75faa47388edcf5    At what age do most Finns master English today?   \n4  2a7b76a679cadb0a016e  What is cheapest place to live in India for on...   \n\n   target  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6f47b0f60633c2056455</td>\n      <td>How can I reply to this comment, \"India is poo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d49b3966070b27bf07fc</td>\n      <td>What did they use for transportation in Ancien...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6d5faa49380557c8ca7b</td>\n      <td>What are the most important provisions of Obam...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cebea75faa47388edcf5</td>\n      <td>At what age do most Finns master English today?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2a7b76a679cadb0a016e</td>\n      <td>What is cheapest place to live in India for on...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_dset_df.head()"
   ]
  },
  {
   "source": [
    "## 2. Steps of pre-embedding preprocessing:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'How can I reply to this comment, \"India is poor. It is a fact. I don\\'t understand the unnecessary criticism of Snapchat CEO\\'s statement\"?'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "sample_sentence = train_dset_df.question_text[0]\n",
    "sample_sentence"
   ]
  },
  {
   "source": [
    "### 2.1. Lowercasing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'how can i reply to this comment, \"india is poor. it is a fact. i don\\'t understand the unnecessary criticism of snapchat ceo\\'s statement\"?'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "sample_sentence = sample_sentence.lower()\n",
    "sample_sentence"
   ]
  },
  {
   "source": [
    "### 2.2. Contractions removal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'how can i reply to this comment, \"india is poor. it is a fact. i do not understand the unnecessary criticism of snapchat ceo\\'s statement\"?'"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sample_sentence = contractions.fix(sample_sentence) \n",
    "sample_sentence"
   ]
  },
  {
   "source": [
    "### 2.3. Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['how',\n 'can',\n 'i',\n 'reply',\n 'to',\n 'this',\n 'comment',\n 'india',\n 'is',\n 'poor',\n 'it',\n 'is',\n 'a',\n 'fact',\n 'i',\n 'do',\n 'not',\n 'understand',\n 'the',\n 'unnecessary',\n 'criticism',\n 'of',\n 'snapchat',\n 'ceo',\n 's',\n 'statement']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "basic_tok = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "sample_sentence = basic_tok.tokenize(sample_sentence)\n",
    "sample_sentence"
   ]
  },
  {
   "source": [
    "### 2.4. Stop-word removal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['reply',\n 'comment',\n 'india',\n 'poor',\n 'fact',\n 'understand',\n 'unnecessary',\n 'criticism',\n 'snapchat',\n 'ceo',\n 'statement']"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "stopwords_corpus = nltk.corpus.stopwords\n",
    "sample_sentence = [word for word in sample_sentence if not word in stopwords_corpus.words()]\n",
    "sample_sentence"
   ]
  },
  {
   "source": [
    "### 2.5 Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['reply',\n 'comment',\n 'india',\n 'poor',\n 'fact',\n 'understand',\n 'unnecessary',\n 'criticism',\n 'snapchat',\n 'ceo',\n 'statement']"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "sample_sentence = [lemmatizer.lemmatize(word) for word in sample_sentence]\n",
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor_AKB:\n",
    "    def __init__(self):\n",
    "        import nltk\n",
    "        import contractions \n",
    "        self.tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "        self.stopwords_corpus = set(nltk.corpus.stopwords.words())\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    def preprocess(self,sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = contractions.fix(sentence)\n",
    "        sentence = self.tokenizer.tokenize(sentence)\n",
    "        sentence = [word for word in sentence if not word in self.stopwords_corpus]\n",
    "        sentence = sentence = [self.lemmatizer.lemmatize(word) for word in sentence]\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'How can I reply to this comment, \"India is poor. It is a fact. I don\\'t understand the unnecessary criticism of Snapchat CEO\\'s statement\"?'"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "preprocessor = Preprocessor_AKB()\n",
    "sample_sentence_2 = train_dset_df.question_text[0]\n",
    "sample_sentence_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['reply',\n 'comment',\n 'india',\n 'poor',\n 'fact',\n 'understand',\n 'unnecessary',\n 'criticism',\n 'snapchat',\n 'ceo',\n 'statement']"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "preprocessor.preprocess(sample_sentence_2)"
   ]
  },
  {
   "source": [
    "## 3. GloVe Embeddings\n",
    "\n",
    "### 3.1. Importing the embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove_Embedder:\n",
    "    def __init__(self, PATH_TO_TEXTFILE):\n",
    "        self.glove_embeddings_dict = {}\n",
    "        glove_embeddings_file = open(PATH_TO_TEXTFILE, 'r')\n",
    "        firstTime = True\n",
    "        while True:\n",
    "            line = glove_embeddings_file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            splitted = line.split()\n",
    "            key = splitted[0]\n",
    "            value = np.array([float(i) for i in splitted[1:]])\n",
    "            if(firstTime):\n",
    "                firstTime = False \n",
    "                self.embedding_vector_size = value.size\n",
    "            self.glove_embeddings_dict[key] = value\n",
    "        glove_embeddings_file.close()\n",
    "    def get_embedding_for_sentence(self, sentence_list):\n",
    "        '''\n",
    "        The sentence should be lowercased and free of special characters and numbers. Ideally, it should be lemmatized, too. The sentence should be a list of words.\n",
    "        '''\n",
    "        number_of_words = len(sentence_list)\n",
    "        embedding = np.zeros((self.embedding_vector_size, ))\n",
    "        if(number_of_words == 0):\n",
    "            return embedding \n",
    "        for word in sentence_list:\n",
    "            if word in self.glove_embeddings_dict:\n",
    "                embedding += self.glove_embeddings_dict[word]\n",
    "        embedding /= number_of_words\n",
    "        return embedding.tolist()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Glove_Embedder(GLOVE_FOLDER_PATH + \"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Has Greek life changed over years? If yes, how?'"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "sample_sentence_3 = train_dset_df.question_text[101]\n",
    "sample_sentence_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['greek', 'life', 'changed', 'year', 'yes']"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "sample_sentence_3 = preprocessor.preprocess(sample_sentence_3)\n",
    "sample_sentence_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.020713399999999983,\n 0.40521799999999997,\n -0.166104,\n -0.36722,\n 0.4106754,\n 0.1663996,\n -0.5283260000000001,\n -0.29789000000000004,\n -0.6921976800000001,\n 0.11451759999999997,\n 0.12571399999999996,\n 0.196332,\n -0.09415899999999999,\n -0.06583960000000001,\n 0.832236,\n 0.10686359999999999,\n -0.19629939999999999,\n 0.051657600000000005,\n -0.3432042,\n 0.043220000000000015,\n 0.022201000000000005,\n 0.16160839999999999,\n -0.01245400000000001,\n -0.032322800000000006,\n 0.410208,\n -1.43652,\n -0.6133586,\n -0.14951799999999998,\n 0.033224,\n 0.08150400000000005,\n 2.86002,\n 0.1380098,\n -0.23056519999999997,\n -0.04970600000000002,\n -0.03637840000000001,\n -0.266058,\n 0.123546,\n -0.09892679999999997,\n -0.08487340000000002,\n -0.264732,\n -0.41872119999999996,\n -0.11833960000000002,\n 0.1870414,\n 0.028474200000000005,\n -0.323266,\n 0.17178480000000002,\n -0.21671899999999997,\n 0.12431800000000001,\n 0.073934,\n -0.08107039999999996]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "sample_embedding = embedder.get_embedding_for_sentence(sample_sentence_3)\n",
    "sample_embedding"
   ]
  },
  {
   "source": [
    "## 4. Putting it all together to obtain ndarrays "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 783673/783673 [00:25<00:00, 30980.09it/s]\n"
    }
   ],
   "source": [
    "train_dset_df.question_text = train_dset_df.question_text.progress_apply(preprocessor.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 783673/783673 [00:10<00:00, 72238.83it/s]\n"
    }
   ],
   "source": [
    "train_dset_df.question_text = train_dset_df.question_text.progress_apply(embedder.get_embedding_for_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_dset_df.question_text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y = train_dset_df.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[::2,:]\n",
    "train_Y = Y[::2]\n",
    "test_X = X[1::2,:]\n",
    "test_Y = Y[1::2]"
   ]
  },
  {
   "source": [
    "## 5. Training a polynomial-kernel SVM on the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics \n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/home/akb/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression()"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "log_reg = sklearn.linear_model.LogisticRegression()\n",
    "log_reg.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Yhat = log_reg.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9369457197763356"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "train_accuracy = sklearn.metrics.accuracy_score(train_Y, train_Yhat)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Yhat = log_reg.predict(test_X)\n",
    "test_accuracy = sklearn.metrics.accuracy_score(test_Y, test_Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9371931114037505"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model = pickle.dumps(log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['2020_09_28_pickled_model.joblib']"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "joblib.dump(pickled_model,'2020_09_28_pickled_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}