{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit760f76561f6442a8b7e1ff17f4562bdb",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Imports and embedding\n",
    "See 2020_10_28_clustering_and_visualization_glove.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove_Embedder:\n",
    "    def __init__(self, PATH_TO_TEXTFILE):\n",
    "        self.glove_embeddings_dict = {}\n",
    "        glove_embeddings_file = open(PATH_TO_TEXTFILE, 'r')\n",
    "        firstTime = True\n",
    "        while True:\n",
    "            line = glove_embeddings_file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            splitted = line.split()\n",
    "            key = splitted[0]\n",
    "            value = np.array([float(i) for i in splitted[1:]])\n",
    "            if(firstTime):\n",
    "                firstTime = False \n",
    "                self.embedding_vector_size = value.size\n",
    "            self.glove_embeddings_dict[key] = value\n",
    "        glove_embeddings_file.close()\n",
    "    def get_embedding_for_sentence(self, sentence_list):\n",
    "        '''\n",
    "        The sentence should be lowercased and free of special characters and numbers. Ideally, it should be lemmatized, too. The sentence should be a list of words.\n",
    "        '''\n",
    "        number_of_words = len(sentence_list)\n",
    "        embedding = np.zeros((self.embedding_vector_size, ))\n",
    "        if(number_of_words == 0):\n",
    "            return embedding \n",
    "        for word in sentence_list:\n",
    "            if word in self.glove_embeddings_dict:\n",
    "                embedding += self.glove_embeddings_dict[word]\n",
    "        embedding /= number_of_words\n",
    "        return embedding.tolist()\n",
    "    def get_embedding_for_word(self, word):\n",
    "        if word in self.glove_embeddings_dict:\n",
    "            embedding = self.glove_embeddings_dict[word]\n",
    "        else:\n",
    "            embedding = np.zeros((self.embedding_vector_size, ))\n",
    "        return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gc\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "train_dset_df = pd.read_csv(\"2020_10_19_train_dset_df_nostem_nostoprem.csv\")\n",
    "test_dset_df = pd.read_csv(\"2020_10_19_test_dset_df_nostem_nostoprem.csv\")\n",
    "train_dset_df[\"preprocessed_joined\"].fillna(\"\", inplace=True)\n",
    "test_dset_df[\"preprocessed_joined\"].fillna(\"\", inplace=True)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_dset_df[\"preprocessed_joined\"])\n",
    "original_sparse_train_x = vectorizer.transform(train_dset_df[\"preprocessed_joined\"])\n",
    "original_sparse_test_x  = vectorizer.transform(test_dset_df[\"preprocessed_joined\"])\n",
    "train_dset_y = train_dset_df[\"target\"].to_numpy()\n",
    "\n",
    "def summarize(y, yhat):\n",
    "    '''\n",
    "    y and yhat are both 1-dimensional ndarrays where every entry is either 0 or 1. \n",
    "    y and yhat must have the same size \n",
    "    '''\n",
    "    print(\"Number of zeros in y:\", np.sum( (y == 0).astype(int) ))\n",
    "    print(\" Number of ones in y:\", np.sum((y == 1).astype(int)))\n",
    "    print(\"            F1 score:\", f1_score(y, yhat))\n",
    "    print(\" # of zeros wrong yh:\", np.sum(np.logical_and(y == 0, yhat == 1).astype(int)))\n",
    "    print(\"  # of ones wrong yh:\", np.sum(np.logical_and(y == 1, yhat == 0).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 783673/783673 [00:02<00:00, 357938.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "class Stopword_Remover:\n",
    "    def __init__(self):\n",
    "        self.stopwordCorpus = set(nltk.corpus.stopwords.words())\n",
    "    def stopword_removed(self, sentence_str):\n",
    "        return \" \".join([word for word in sentence_str.split(\" \") if not word in self.stopwordCorpus])\n",
    "srem = Stopword_Remover()\n",
    "train_dset_df[\"preprocessed_joined\"] = train_dset_df[\"preprocessed_joined\"].progress_apply(srem.stopword_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_list = sorted(list(set((\" \".join(train_dset_df[\"preprocessed_joined\"].tolist())).split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "54264"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Glove_Embedder(\"./embeddings/glove/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_words_list = []\n",
    "embeddings_list = []\n",
    "for word in unique_words_list:\n",
    "    curr_embedding = embedder.get_embedding_for_word(word)\n",
    "    if not(np.all(np.array(curr_embedding) == 0)):\n",
    "        embedded_words_list.append(word)\n",
    "        embeddings_list.append(curr_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "52857"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(embedded_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "52857"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(embeddings_list)"
   ]
  },
  {
   "source": [
    "## Clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_nd = np.array(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggcls = AgglomerativeClustering(n_clusters=10, linkage=\"ward\", affinity=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = aggcls.fit_predict(embeddings_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 5, 7, ..., 3, 3, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "source": [
    "Well, that worked. Even though it took 26GB to handle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupable_dict =   {\"words\": embedded_words_list, \"labels\":labels.tolist() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(groupable_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  words  labels\n",
       "0   aaa       0\n",
       "1  aaaa       5\n",
       "2   aab       7\n",
       "3   aac       3\n",
       "4  aach       2"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>words</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aaa</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aaaa</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aab</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aac</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aach</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_labels_df = labels_df.groupby(by=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in grouped_labels_df:\n",
    "    with open(\"2020_11_5_agglo_group_\"+str(name)+\".txt\", \"w\") as file_handle:\n",
    "        words_str = \"\\n\".join(group[\"words\"].to_list())\n",
    "        file_handle.write(words_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}