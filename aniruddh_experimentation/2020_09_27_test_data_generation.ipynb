{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit760f76561f6442a8b7e1ff17f4562bdb",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Acting on test data, using the AKB 2020_09_27 model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSET_FOLDER_PATH = './dataset/quora/'\n",
    "GLOVE_FOLDER_PATH = './embeddings/glove/'\n",
    "MODEL_PATH = \"pickledModels/2020_09_28_pickled_model.joblib\""
   ]
  },
  {
   "source": [
    "## 1. Collecting NLTK and the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df = pd.read_csv(DSET_FOLDER_PATH + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                    qid                                      question_text\n0  f56a9a31974dc66186e8  Is it a good idea to go through a phlebotomy c...\n1  d957c3758060f45da303  How can I fix a lead into a camlin compass to ...\n2  ad822d5abaedb9e247b9                How many animes are there in world?\n3  4e979c23eeb6a4bd1f2e                     How do I tell my family I cut?\n4  333cc031262566b8da49  How do I save down my bitcoin image address fr...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f56a9a31974dc66186e8</td>\n      <td>Is it a good idea to go through a phlebotomy c...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d957c3758060f45da303</td>\n      <td>How can I fix a lead into a camlin compass to ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ad822d5abaedb9e247b9</td>\n      <td>How many animes are there in world?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4e979c23eeb6a4bd1f2e</td>\n      <td>How do I tell my family I cut?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>333cc031262566b8da49</td>\n      <td>How do I save down my bitcoin image address fr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "source": [
    "## 2. Steps of pre-embedding preprocessing:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor_AKB:\n",
    "    def __init__(self):\n",
    "        import nltk\n",
    "        import contractions \n",
    "        self.tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "        self.stopwords_corpus = set(nltk.corpus.stopwords.words())\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    def preprocess(self,sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = contractions.fix(sentence)\n",
    "        sentence = self.tokenizer.tokenize(sentence)\n",
    "        sentence = [word for word in sentence if not word in self.stopwords_corpus]\n",
    "        sentence = sentence = [self.lemmatizer.lemmatize(word) for word in sentence]\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor_AKB()"
   ]
  },
  {
   "source": [
    "## 3. GloVe Embeddings\n",
    "\n",
    "### 3.1. Importing the embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove_Embedder:\n",
    "    def __init__(self, PATH_TO_TEXTFILE):\n",
    "        self.glove_embeddings_dict = {}\n",
    "        glove_embeddings_file = open(PATH_TO_TEXTFILE, 'r')\n",
    "        firstTime = True\n",
    "        while True:\n",
    "            line = glove_embeddings_file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            splitted = line.split()\n",
    "            key = splitted[0]\n",
    "            value = np.array([float(i) for i in splitted[1:]])\n",
    "            if(firstTime):\n",
    "                firstTime = False \n",
    "                self.embedding_vector_size = value.size\n",
    "            self.glove_embeddings_dict[key] = value\n",
    "        glove_embeddings_file.close()\n",
    "    def get_embedding_for_sentence(self, sentence_list):\n",
    "        '''\n",
    "        The sentence should be lowercased and free of special characters and numbers. Ideally, it should be lemmatized, too. The sentence should be a list of words.\n",
    "        '''\n",
    "        number_of_words = len(sentence_list)\n",
    "        embedding = np.zeros((self.embedding_vector_size, ))\n",
    "        if(number_of_words == 0):\n",
    "            return embedding \n",
    "        for word in sentence_list:\n",
    "            if word in self.glove_embeddings_dict:\n",
    "                embedding += self.glove_embeddings_dict[word]\n",
    "        embedding /= number_of_words\n",
    "        return embedding.tolist()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Glove_Embedder(GLOVE_FOLDER_PATH + \"glove.6B.50d.txt\")"
   ]
  },
  {
   "source": [
    "## 4. Putting it all together to obtain ndarrays "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 522449/522449 [00:19<00:00, 26710.31it/s]\n"
    }
   ],
   "source": [
    "test_data_df.question_text = test_data_df.question_text.progress_apply(preprocessor.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 522449/522449 [00:07<00:00, 73387.68it/s]\n"
    }
   ],
   "source": [
    "test_data_df.question_text = test_data_df.question_text.progress_apply(embedder.get_embedding_for_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array(test_data_df.question_text.to_list())"
   ]
  },
  {
   "source": [
    "## 5. Training a polynomial-kernel SVM on the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model = joblib.load(MODEL_PATH)\n",
    "log_reg = pickle.loads(pickled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model \n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Yhat = log_reg.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0, ..., 0, 0, 0])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "test_Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_df.question_text = test_Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset_df = test_dset_df.rename(columns={\"qid\":\"qid\", \"question_text\":\"target\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.to_csv(\"2020_09_28_testset_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}